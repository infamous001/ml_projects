{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9ee885e4b47b476f86151bf82c07e155":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cecdf4f07a674b77ba29fba9e8acb6a7","IPY_MODEL_74fec94c1a5d4ca58f75ba7e88e9c5a4","IPY_MODEL_a1f6d9cdb3884c57b1f596021ea2ef17"],"layout":"IPY_MODEL_b5fac7368556430ea400e501158d64cf"}},"cecdf4f07a674b77ba29fba9e8acb6a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af24c02ce1fb45ae8cbccc5d45a61c85","placeholder":"​","style":"IPY_MODEL_4ae2b18a249e462d95a7715cdd3a3ebd","value":"Downloading data: 100%"}},"74fec94c1a5d4ca58f75ba7e88e9c5a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d3c29a07302416892014420f136fd7a","max":5726189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4735dd605f224046b16ecf97175ccfe2","value":5726189}},"a1f6d9cdb3884c57b1f596021ea2ef17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90d9d23b4ff3433a95f15bf2bf7d36e6","placeholder":"​","style":"IPY_MODEL_f3f488c928d24e88a675375ff2bfaf3f","value":" 5.73M/5.73M [00:01&lt;00:00, 3.72MB/s]"}},"b5fac7368556430ea400e501158d64cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af24c02ce1fb45ae8cbccc5d45a61c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae2b18a249e462d95a7715cdd3a3ebd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d3c29a07302416892014420f136fd7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4735dd605f224046b16ecf97175ccfe2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90d9d23b4ff3433a95f15bf2bf7d36e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3f488c928d24e88a675375ff2bfaf3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f645c31a5f784d96b6607df566f54334":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f651dd5bc0bb438caf7d20abbdecf53e","IPY_MODEL_0128cc27cd2b4beaaae6e3e5dbca8597","IPY_MODEL_5165dbcf38864f2f9fae54051881e5b4"],"layout":"IPY_MODEL_1e2222eaf76a4e14a36e72af937f1ef8"}},"f651dd5bc0bb438caf7d20abbdecf53e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2516bea03ca4f709843f785b98065d7","placeholder":"​","style":"IPY_MODEL_a1b067df9d5341ff8e86f06d3ff1e332","value":"Generating train split: 100%"}},"0128cc27cd2b4beaaae6e3e5dbca8597":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1225f9ce9f0e4cb5b66391f5983af3f5","max":32332,"min":0,"orientation":"horizontal","style":"IPY_MODEL_febb00749a0d4426ac2b330900c5a6cd","value":32332}},"5165dbcf38864f2f9fae54051881e5b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f735649f6da4006bb27c34e4ac28fc0","placeholder":"​","style":"IPY_MODEL_a481616a7f1e4ddf890bce944ccb58ef","value":" 32332/32332 [00:00&lt;00:00, 214514.24 examples/s]"}},"1e2222eaf76a4e14a36e72af937f1ef8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2516bea03ca4f709843f785b98065d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1b067df9d5341ff8e86f06d3ff1e332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1225f9ce9f0e4cb5b66391f5983af3f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febb00749a0d4426ac2b330900c5a6cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f735649f6da4006bb27c34e4ac28fc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a481616a7f1e4ddf890bce944ccb58ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install Tokenizer  Datasets torchmetrics","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqQgOvWIF1oj","outputId":"73f25b46-5806-4aff-909b-5609a6041938","execution":{"iopub.status.busy":"2024-01-11T11:41:51.731243Z","iopub.execute_input":"2024-01-11T11:41:51.731909Z","iopub.status.idle":"2024-01-11T11:42:03.827397Z","shell.execute_reply.started":"2024-01-11T11:41:51.731877Z","shell.execute_reply":"2024-01-11T11:42:03.826261Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: Tokenizer in /opt/conda/lib/python3.10/site-packages (3.4.3)\nRequirement already satisfied: Datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.2.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from Datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from Datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from Datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from Datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from Datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from Datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from Datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from Datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from Datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from Datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from Datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from Datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from Datasets) (0.18.0)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.10.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->Datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->Datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->Datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->Datasets) (4.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (68.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->Datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->Datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->Datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->Datasets) (2023.11.17)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->Datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->Datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"id":"M-OWuOR0GI2z","execution":{"iopub.status.busy":"2024-01-11T11:42:03.829403Z","iopub.execute_input":"2024-01-11T11:42:03.829713Z","iopub.status.idle":"2024-01-11T11:42:03.834338Z","shell.execute_reply.started":"2024-01-11T11:42:03.829686Z","shell.execute_reply":"2024-01-11T11:42:03.833351Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import math\nclass InputEmbedding(nn.Module):\n  def __init__(self,d_model:int,vocab_size:int):\n    super().__init__()\n    self.d_model=d_model\n    self.input_embedding=nn.Embedding(vocab_size,d_model)\n  def forward(self,x):\n    return self.input_embedding(x)*math.sqrt(self.d_model)","metadata":{"id":"CskHnsuAGP_u","execution":{"iopub.status.busy":"2024-01-11T11:42:18.728686Z","iopub.execute_input":"2024-01-11T11:42:18.729324Z","iopub.status.idle":"2024-01-11T11:42:18.735506Z","shell.execute_reply.started":"2024-01-11T11:42:18.729291Z","shell.execute_reply":"2024-01-11T11:42:18.734382Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n  def __init__(self,d_model:int,max_seq_len:int,dropout:float):\n    super().__init__()\n    self.dropout=nn.Dropout(dropout)\n\n    pe=torch.zeros(max_seq_len,d_model)\n    position=torch.arange(0,max_seq_len,dtype=torch.float).unsqueeze(1)\n    divide_term=torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n    pe[:,0::2]=torch.sin(position*divide_term)\n    pe[:,1::2]=torch.cos(position*divide_term)\n    pe=pe.unsqueeze(0)\n    self.register_buffer('pe',pe)\n\n  def forward(self,x):\n    x= x+(self.pe[:,:x.shape[1],:]).requires_grad_(False)\n    return self.dropout(x)","metadata":{"id":"4x1SzsUZIN4u","execution":{"iopub.status.busy":"2024-01-11T11:42:20.696510Z","iopub.execute_input":"2024-01-11T11:42:20.697393Z","iopub.status.idle":"2024-01-11T11:42:20.708114Z","shell.execute_reply.started":"2024-01-11T11:42:20.697359Z","shell.execute_reply":"2024-01-11T11:42:20.707119Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n  def __init__(self,d_model:int,head:int,dropout:float):\n    super().__init__()\n    self.head=head\n    self.dk=d_model//head\n    self.wq=nn.Linear(d_model,d_model)\n    self.wk=nn.Linear(d_model,d_model)\n    self.wv=nn.Linear(d_model,d_model)\n    self.wo=nn.Linear(d_model,d_model)\n    self.dropout=nn.Dropout(dropout)\n  @staticmethod\n  def attention(query,key,value,mask,dropout:nn.Dropout):\n    d_k=query.shape[-1]\n    attention_score=(query@key.transpose(-2,-1))/math.sqrt(d_k)\n    if mask is not None:\n      attention_score.masked_fill_(mask==0,-1e7)\n    attention_score=dropout(attention_score)\n\n    return (attention_score@value),attention_score\n  def forward(self,q,k,v,mask):\n    query=self.wq(q)\n    key=self.wk(k)\n    value=self.wv(v)\n\n    query=query.view(query.shape[0],query.shape[1],self.head,self.dk).transpose(1,2)\n    key=key.view(key.shape[0],key.shape[1],self.head,self.dk).transpose(1,2)\n    value=value.view(value.shape[0],value.shape[1],self.head,self.dk).transpose(1,2)\n\n    x,self.attention_score=MultiHeadAttention.attention(query,key,value,mask,self.dropout)\n    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.head*self.dk)\n\n    return self.wo(x)","metadata":{"id":"zGyXWv1-OgP6","execution":{"iopub.status.busy":"2024-01-11T11:42:23.632703Z","iopub.execute_input":"2024-01-11T11:42:23.633514Z","iopub.status.idle":"2024-01-11T11:42:23.644958Z","shell.execute_reply.started":"2024-01-11T11:42:23.633484Z","shell.execute_reply":"2024-01-11T11:42:23.643872Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n  def __init__(self,d_model:int,d_ff:int,dropout:float):\n    super().__init__()\n    self.layer_1=nn.Linear(d_model,d_ff)\n    self.layer_2=nn.Linear(d_ff,d_model)\n    self.dropout=nn.Dropout(dropout)\n  def forward(self,x):\n    return self.dropout(self.layer_2(torch.relu(self.layer_1(x))))","metadata":{"id":"50gWBiDqWxaf","execution":{"iopub.status.busy":"2024-01-11T11:42:24.109415Z","iopub.execute_input":"2024-01-11T11:42:24.109751Z","iopub.status.idle":"2024-01-11T11:42:24.115773Z","shell.execute_reply.started":"2024-01-11T11:42:24.109724Z","shell.execute_reply":"2024-01-11T11:42:24.114774Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n  def __init__(self,epsilon:float=10**-5):\n    super().__init__()\n    self.epsilon=epsilon\n  def forward(self,x):\n    mean=x.mean(dim=-1,keepdims=True)\n    std=x.std(dim=-1,keepdims=True)\n    return (x-mean)/(std+self.epsilon)","metadata":{"id":"EHM-IUxEU0OP","execution":{"iopub.status.busy":"2024-01-11T11:42:24.572923Z","iopub.execute_input":"2024-01-11T11:42:24.573306Z","iopub.status.idle":"2024-01-11T11:42:24.579570Z","shell.execute_reply.started":"2024-01-11T11:42:24.573276Z","shell.execute_reply":"2024-01-11T11:42:24.578586Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n  def __init__(self,dropout):\n    super().__init__()\n    self.dropout=nn.Dropout(dropout)\n    self.layer_norm=LayerNormalization()\n  def forward(self,x,sublayer):\n    return x+ self.dropout(self.layer_norm(sublayer(x)))","metadata":{"id":"tuTOfPssYe_R","execution":{"iopub.status.busy":"2024-01-11T11:42:25.101092Z","iopub.execute_input":"2024-01-11T11:42:25.101497Z","iopub.status.idle":"2024-01-11T11:42:25.107168Z","shell.execute_reply.started":"2024-01-11T11:42:25.101466Z","shell.execute_reply":"2024-01-11T11:42:25.106177Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n  def __init__(self,d_model:int,vocab_size:int):\n    super().__init__()\n    self.projection_layer=nn.Linear(d_model,vocab_size)\n  def forward(self,x):\n    return torch.log_softmax(self.projection_layer(x),dim=-1)","metadata":{"id":"PPvcwa6Dam7e","execution":{"iopub.status.busy":"2024-01-11T11:42:25.528567Z","iopub.execute_input":"2024-01-11T11:42:25.528915Z","iopub.status.idle":"2024-01-11T11:42:25.534550Z","shell.execute_reply.started":"2024-01-11T11:42:25.528887Z","shell.execute_reply":"2024-01-11T11:42:25.533538Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n  def __init__(self,self_attention:MultiHeadAttention,feed_forward:FeedForward,dropout:float):\n    super().__init__()\n    self.self_attention=self_attention\n    self.feed_forward=feed_forward\n\n    self.residual=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n  def forward(self,x,src_mask):\n    x=self.residual[0](x,lambda x:self.self_attention(x,x,x,src_mask))\n    x=self.residual[1](x,self.feed_forward)\n\n    return x\n\nclass Encoder(nn.Module):\n  def __init__(self,layers:nn.ModuleList):\n    super().__init__()\n    self.layer=layers\n    self.norm=LayerNormalization()\n  def forward(self,x,mask):\n    for l in self.layer:\n      x=l(x,mask)\n    return self.norm(x)","metadata":{"id":"sZ-_TEhgaw-E","execution":{"iopub.status.busy":"2024-01-11T11:42:25.997263Z","iopub.execute_input":"2024-01-11T11:42:25.997962Z","iopub.status.idle":"2024-01-11T11:42:26.006020Z","shell.execute_reply.started":"2024-01-11T11:42:25.997932Z","shell.execute_reply":"2024-01-11T11:42:26.005078Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n  def __init__(self,self_attention:MultiHeadAttention,cross_attention:MultiHeadAttention,feed_forward:FeedForward,dropout:float):\n    super().__init__()\n    self.self_attention=self_attention\n    self.cross_attention=cross_attention\n    self.feed_forward=feed_forward\n\n    self.residual=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n  def forward(self,x,encoder_output,src_mask,trg_mask):\n    x=self.residual[0](x,lambda x:self.self_attention(x,x,x,trg_mask))\n    x=self.residual[1](x,lambda x:self.cross_attention(x,encoder_output,encoder_output,src_mask))\n    x=self.residual[2](x,self.feed_forward)\n\n    return x\n\nclass Decoder(nn.Module):\n  def __init__(self,layers:nn.ModuleList):\n    super().__init__()\n    self.layer=layers\n    self.norm=LayerNormalization()\n  def forward(self,x,encoder_output,src_mask,trg_mask):\n    for l in self.layer:\n      x=l(x,encoder_output,src_mask,trg_mask)\n    return self.norm(x)","metadata":{"id":"KLiIxgWffCd2","execution":{"iopub.status.busy":"2024-01-11T11:42:26.678798Z","iopub.execute_input":"2024-01-11T11:42:26.679174Z","iopub.status.idle":"2024-01-11T11:42:26.688469Z","shell.execute_reply.started":"2024-01-11T11:42:26.679143Z","shell.execute_reply":"2024-01-11T11:42:26.687613Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Transformer(nn.Module):\n  def __init__(self,src_embedding:InputEmbedding,trg_embedding:InputEmbedding,src_position:PositionalEncoding,trg_position:PositionalEncoding,encoder:Encoder,decoder:Decoder,projection_layer:ProjectionLayer):\n    super().__init__()\n    self.src_embedding=src_embedding\n    self.src_position=src_position\n    self.encoder=encoder\n    self.trg_embedding=trg_embedding\n    self.trg_position=trg_position\n    self.decoder=decoder\n    self.projection_layer=projection_layer\n  def encode(self,src,src_mask):\n    src=self.src_embedding(src)\n    src=self.src_position(src)\n    return self.encoder(src,src_mask)\n  def decode(self,trg,encoder_output,src_mask,trg_mask):\n    trg=self.trg_embedding(trg)\n    trg=self.trg_position(trg)\n    return self.decoder(trg,encoder_output,src_mask,trg_mask)\n  def project(self,x):\n    return self.projection_layer(x)\n\ndef BuildTransformer(src_vocab_size:int,trg_vocab_size:int,src_max_seq_len:int,trg_max_seq_len:int,d_model:int=512,N:int=4,head:int=4,dropout:float=0.1,d_ff:int=2048):\n  src_embedding=InputEmbedding(d_model,src_vocab_size)\n  src_position=PositionalEncoding(d_model,src_max_seq_len,dropout)\n\n  encoder_array=[]\n  for _ in range(N):\n    encoder_attention=MultiHeadAttention(d_model,head,dropout)\n    encoder_feed_forward=FeedForward(d_model,d_ff,dropout)\n    encoder_block=EncoderBlock(encoder_attention,encoder_feed_forward,dropout)\n    encoder_array.append(encoder_block)\n  encoder=Encoder(nn.ModuleList(encoder_array))\n\n  trg_embedding=InputEmbedding(d_model,trg_vocab_size)\n  trg_position=PositionalEncoding(d_model,trg_max_seq_len,dropout)\n\n  decoder_array=[]\n  for _ in range(N):\n    decoder_self_attention=MultiHeadAttention(d_model,head,dropout)\n    decoder_cross_attention=MultiHeadAttention(d_model,head,dropout)\n    decoder_feed_forward=FeedForward(d_model,d_ff,dropout)\n    decoder_block=DecoderBlock(decoder_self_attention,decoder_cross_attention,decoder_feed_forward,dropout)\n    decoder_array.append(decoder_block)\n  decoder=Decoder(nn.ModuleList(decoder_array))\n\n  projection_layer=ProjectionLayer(d_model,trg_vocab_size)\n\n  transformer=Transformer(src_embedding,trg_embedding,src_position,trg_position,encoder,decoder,projection_layer)\n\n  for p in transformer.parameters():\n    if p.dim()>1:\n      nn.init.xavier_uniform_(p)\n  return transformer","metadata":{"id":"T959mUothH4l","execution":{"iopub.status.busy":"2024-01-11T11:42:27.348846Z","iopub.execute_input":"2024-01-11T11:42:27.349396Z","iopub.status.idle":"2024-01-11T11:42:27.366248Z","shell.execute_reply.started":"2024-01-11T11:42:27.349356Z","shell.execute_reply":"2024-01-11T11:42:27.365164Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import BPE\nfrom tokenizers.trainers import BpeTrainer\nfrom tokenizers.pre_tokenizers import Whitespace","metadata":{"id":"uEdiVPFwjidg","execution":{"iopub.status.busy":"2024-01-11T11:42:27.877617Z","iopub.execute_input":"2024-01-11T11:42:27.878416Z","iopub.status.idle":"2024-01-11T11:42:29.634221Z","shell.execute_reply.started":"2024-01-11T11:42:27.878385Z","shell.execute_reply":"2024-01-11T11:42:29.633336Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\ndef get_all_sentences(ds,lang):\n  for item in ds:\n    yield item[\"translation\"][lang]\n\ndef build_tokenizer(config,ds,lang):\n  tokenizer_path=Path(config[\"tokenizer_file\"].format(lang))#file name to save json file of saved tokenizer\n  if not Path.exists(tokenizer_path):#if tokenizer not exist lets make one\n    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))#initilize the tokenizer\n    tokenizer.pre_tokenizer=Whitespace()#to seprate the sentence to words so no token is greater than a word\n    trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\",\"[EOS]\"],min_frequency=2)#initilize trainer\n    tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)#train from a list given one by one by get_all_sentence\n    tokenizer.save(str(tokenizer_path))#save the model in current director with this name\n  else:\n    tokenizer=Tokenizer.from_file(str(tokenizer_path))#if tokeizer exist then call it with file name\n  return tokenizer","metadata":{"id":"wghWTl7d1unS","execution":{"iopub.status.busy":"2024-01-11T11:42:30.592195Z","iopub.execute_input":"2024-01-11T11:42:30.592952Z","iopub.status.idle":"2024-01-11T11:42:30.601097Z","shell.execute_reply.started":"2024-01-11T11:42:30.592905Z","shell.execute_reply":"2024-01-11T11:42:30.600154Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class BilingualDataset(nn.Module):\n  def __init__(self,ds,src_tokenizer,trg_tokenizer,max_seq_len,src_lang,trg_lang):\n    super().__init__()\n    self.ds=ds\n    self.src_tokenizer=src_tokenizer\n    self.trg_tokenizer=trg_tokenizer\n    self.max_seq_len=max_seq_len\n    self.src_lang=src_lang\n    self.trg_lang=trg_lang\n\n    self.sos_token=torch.tensor([src_tokenizer.token_to_id(\"[SOS]\")],dtype=torch.int64)\n    self.eos_token=torch.tensor([src_tokenizer.token_to_id(\"[EOS]\")],dtype=torch.int64)\n    self.pad_token=torch.tensor([src_tokenizer.token_to_id(\"[PAD]\")],dtype=torch.int64)\n\n  def __len__(self):\n    return len(self.ds)\n\n  def __getitem__(self,index):\n    pair_text=self.ds[index]\n\n    src_text=pair_text[\"translation\"][self.src_lang]\n    trg_text=pair_text[\"translation\"][self.trg_lang]\n\n    encoder_input_tokens=self.src_tokenizer.encode(src_text).ids\n    decoder_input_tokens=self.trg_tokenizer.encode(trg_text).ids\n\n    enc_num_padding_tokens = self.max_seq_len - len(encoder_input_tokens) - 2\n    dec_num_padding_tokens = self.max_seq_len - len(decoder_input_tokens) - 1\n\n    encoder_input=torch.cat([\n        self.sos_token,\n        torch.tensor(encoder_input_tokens,dtype=torch.int64),\n        self.eos_token,\n        torch.tensor([self.pad_token]*enc_num_padding_tokens,dtype=torch.int64)\n    ],dim=0)\n\n    decoder_input=torch.cat([\n        self.sos_token,\n        torch.tensor(decoder_input_tokens,dtype=torch.int64),\n        torch.tensor([self.pad_token]*dec_num_padding_tokens,dtype=torch.int64)\n    ],dim=0)\n\n    label=torch.cat([\n        torch.tensor(decoder_input_tokens,dtype=torch.int64),\n        self.eos_token,\n        torch.tensor([self.pad_token]*dec_num_padding_tokens,dtype=torch.int64),\n    ],dim=0)\n\n    return {\n        \"encoder_input\":encoder_input,\n        \"decoder_input\":decoder_input,\n        \"encoder_mask\":(encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n        \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n        \"label\": label,\n        \"src_text\": src_text,\n        \"trg_text\": trg_text,\n    }\n\ndef causal_mask(size):\n  mask=torch.triu(torch.ones((1,size,size)),diagonal=1).type(torch.int)\n  return mask==0","metadata":{"id":"g5-KT26F52S3","execution":{"iopub.status.busy":"2024-01-11T11:42:31.530056Z","iopub.execute_input":"2024-01-11T11:42:31.530440Z","iopub.status.idle":"2024-01-11T11:42:31.544656Z","shell.execute_reply.started":"2024-01-11T11:42:31.530410Z","shell.execute_reply":"2024-01-11T11:42:31.543748Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader,random_split\ndef get_ds(config):\n  ds_raw=load_dataset('opus_books',f'{config[\"src_lang\"]}-{config[\"trg_lang\"]}',split=\"train\")\n\n  src_tokenizer=build_tokenizer(config,ds_raw,config[\"src_lang\"])\n  trg_tokenizer=build_tokenizer(config,ds_raw,config[\"trg_lang\"])\n\n  train_ds_size=int(0.9*len(ds_raw))\n  val_ds_size=len(ds_raw)-train_ds_size\n  train_ds_raw,val_ds_raw=random_split(ds_raw,[train_ds_size,val_ds_size])\n\n  train_ds=BilingualDataset(train_ds_raw,src_tokenizer,trg_tokenizer,config['seq_len'],config[\"src_lang\"],config['trg_lang'])\n  val_ds=BilingualDataset(val_ds_raw,src_tokenizer,trg_tokenizer,config['seq_len'],config[\"src_lang\"],config['trg_lang'])\n\n  max_len_src = 0\n  max_len_tgt = 0\n\n  for item in ds_raw:\n    src_ids = src_tokenizer.encode(item['translation'][config['src_lang']]).ids\n    tgt_ids = trg_tokenizer.encode(item['translation'][config['trg_lang']]).ids\n    max_len_src = max(max_len_src, len(src_ids))\n    max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n  print(f'Max length of source sentence: {max_len_src}')\n  print(f'Max length of target sentence: {max_len_tgt}')\n\n  train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n  val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n\n  return train_dataloader, val_dataloader, src_tokenizer, trg_tokenizer\n\ndef get_model(config, vocab_src_len, vocab_tgt_len):\n    model = BuildTransformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n    return model","metadata":{"id":"resx3mvyBAjB","execution":{"iopub.status.busy":"2024-01-11T11:42:34.481945Z","iopub.execute_input":"2024-01-11T11:42:34.482326Z","iopub.status.idle":"2024-01-11T11:42:34.492873Z","shell.execute_reply.started":"2024-01-11T11:42:34.482294Z","shell.execute_reply":"2024-01-11T11:42:34.491877Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def get_config():\n    return{\n        \"batch_size\":24,\n        \"num_epochs\":100,\n        \"lr\":10**-4,\n        \"seq_len\":350,\n        \"d_model\":512,\n        \"datasource\": 'opus_books',\n        \"src_lang\": \"en\",\n        \"trg_lang\": \"it\",\n        \"model_folder\": \"weights\",\n        \"model_basename\": \"tmodel_\",\n        \"preload\": None,\n        \"tokenizer_file\": \"tokenizer_{0}.json\",\n        \"experiment_name\": \"runs/tmodel\"\n    }\ndef get_weights_file_path(config, epoch: str):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)\n\n# Find the latest weights file in the weights folder\ndef latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    if len(weights_files) == 0:\n        return None\n    weights_files.sort()\n    return str(weights_files[-1])","metadata":{"id":"-ATKKPRH1lNF","execution":{"iopub.status.busy":"2024-01-11T11:42:36.646299Z","iopub.execute_input":"2024-01-11T11:42:36.646681Z","iopub.status.idle":"2024-01-11T11:42:36.654734Z","shell.execute_reply.started":"2024-01-11T11:42:36.646650Z","shell.execute_reply":"2024-01-11T11:42:36.653772Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torchmetrics\ndef greedy_decode(model,source,source_mask,src_tokenizer,trg_tokenizer,max_len,device):\n  sos_idx = src_tokenizer.token_to_id('[SOS]')\n  eos_idx = src_tokenizer.token_to_id('[EOS]')\n\n  encoder_output = model.encode(source, source_mask)\n  decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n  while True:\n    if decoder_input.size(1) == max_len:\n        break\n\n    decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n    out = model.decode(decoder_input,encoder_output, source_mask, decoder_mask)\n\n    prob = model.project(out[:, -1])\n    _, next_word = torch.max(prob, dim=1)\n    decoder_input = torch.cat(\n        [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1\n    )\n\n    if next_word == eos_idx:\n      break\n  return decoder_input.squeeze(0)\n\ndef run_validation(model,validation_ds,src_tokenizer,trg_tokenizer,max_len,print_msg,global_steps,device,num_examples=2):\n  model.eval()\n  count=0\n\n  source_texts=[]\n  expected=[]\n  predicted=[]\n\n  with torch.no_grad():\n    for batch in validation_ds:\n      count+=1\n      encoder_input=batch['encoder_input'].to(device)\n      encoder_mask=batch['encoder_mask'].to(device)\n\n      model_out=greedy_decode(model,encoder_input,encoder_mask,src_tokenizer,trg_tokenizer,max_len,device)\n\n      source_text = batch[\"src_text\"][0]\n      target_text = batch[\"trg_text\"][0]\n      model_out_text = trg_tokenizer.decode(model_out.detach().cpu().numpy())\n\n      source_texts.append(source_text)\n      expected.append(target_text)\n      predicted.append(model_out_text)\n\n      print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n      print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n      print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n\n      if count == num_examples:\n        metric = torchmetrics.BLEUScore()\n        bleu = metric(predicted, expected)\n        print_msg(f\"{f'BLEU: ':>12}{bleu}\")\n        break","metadata":{"id":"cQ2jR48f2oa1","execution":{"iopub.status.busy":"2024-01-11T11:42:39.419966Z","iopub.execute_input":"2024-01-11T11:42:39.420663Z","iopub.status.idle":"2024-01-11T11:42:42.043232Z","shell.execute_reply.started":"2024-01-11T11:42:39.420633Z","shell.execute_reply":"2024-01-11T11:42:42.042435Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\ndef train_model(config):\n  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n  print(device)\n  Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n  train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt=get_ds(config)\n  model=get_model(config,tokenizer_src.get_vocab_size(),tokenizer_tgt.get_vocab_size()).to(device)\n  writer=SummaryWriter(config['experiment_name'])\n  optimizer=torch.optim.Adam(model.parameters(),lr=config['lr'],eps=1e-9)\n\n  initial_epoch=0\n  global_step=0\n  preload = config['preload']\n  model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n  if model_filename:\n    print(f'Preloading model {model_filename}')\n    state = torch.load(model_filename)\n    model.load_state_dict(state['model_state_dict'])\n    initial_epoch = state['epoch'] + 1\n    optimizer.load_state_dict(state['optimizer_state_dict'])\n    global_step = state['global_step']\n  else:\n      print('No model to preload, starting from scratch')\n\n  loss_fn=nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n\n  for epoch in range(initial_epoch, config['num_epochs']):\n\n    torch.cuda.empty_cache()\n    model.train()\n    batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n    for batch in batch_iterator:\n\n      encoder_input = batch['encoder_input'].to(device)\n      decoder_input = batch['decoder_input'].to(device)\n      encoder_mask = batch['encoder_mask'].to(device)\n      decoder_mask = batch['decoder_mask'].to(device)\n\n      encoder_output = model.encode(encoder_input, encoder_mask)\n      decoder_output = model.decode(decoder_input,encoder_output, encoder_mask, decoder_mask)\n      proj_output = model.project(decoder_output)\n\n      label = batch['label'].to(device)\n\n      loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n      batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n\n      writer.add_scalar('train loss', loss.item(), global_step)\n      writer.flush()\n\n      loss.backward()\n\n      optimizer.step()\n      optimizer.zero_grad(set_to_none=True)\n      global_step+=1\n\n    run_validation(model,val_dataloader,tokenizer_src,tokenizer_tgt,config['seq_len'],lambda msg:batch_iterator.write(msg),global_step,device)\n\n    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n    torch.save({\n          'epoch': epoch,\n          'model_state_dict': model.state_dict(),\n          'optimizer_state_dict': optimizer.state_dict(),\n          'global_step': global_step\n      }, model_filename)\n","metadata":{"id":"pmCSUfUqgbMA","execution":{"iopub.status.busy":"2024-01-11T11:42:44.753451Z","iopub.execute_input":"2024-01-11T11:42:44.754038Z","iopub.status.idle":"2024-01-11T11:42:56.127699Z","shell.execute_reply.started":"2024-01-11T11:42:44.754003Z","shell.execute_reply":"2024-01-11T11:42:56.126774Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import warnings\nif __name__ == '__main__':\n    warnings.filterwarnings(\"ignore\")\n    config = get_config()\n    train_model(config)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":660,"referenced_widgets":["9ee885e4b47b476f86151bf82c07e155","cecdf4f07a674b77ba29fba9e8acb6a7","74fec94c1a5d4ca58f75ba7e88e9c5a4","a1f6d9cdb3884c57b1f596021ea2ef17","b5fac7368556430ea400e501158d64cf","af24c02ce1fb45ae8cbccc5d45a61c85","4ae2b18a249e462d95a7715cdd3a3ebd","8d3c29a07302416892014420f136fd7a","4735dd605f224046b16ecf97175ccfe2","90d9d23b4ff3433a95f15bf2bf7d36e6","f3f488c928d24e88a675375ff2bfaf3f","f645c31a5f784d96b6607df566f54334","f651dd5bc0bb438caf7d20abbdecf53e","0128cc27cd2b4beaaae6e3e5dbca8597","5165dbcf38864f2f9fae54051881e5b4","1e2222eaf76a4e14a36e72af937f1ef8","a2516bea03ca4f709843f785b98065d7","a1b067df9d5341ff8e86f06d3ff1e332","1225f9ce9f0e4cb5b66391f5983af3f5","febb00749a0d4426ac2b330900c5a6cd","9f735649f6da4006bb27c34e4ac28fc0","a481616a7f1e4ddf890bce944ccb58ef"]},"id":"_c-ZfUE2qlcw","outputId":"26743c3c-1e3f-4dc7-977a-975e48a92ce9","execution":{"iopub.status.busy":"2024-01-11T11:42:56.129111Z","iopub.execute_input":"2024-01-11T11:42:56.129612Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0b7743eedfa4793bfd8bfcad3f0b639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68a3f4196d1451ebab2478404cfc761"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset opus_books/en-it (download: 3.14 MiB, generated: 8.58 MiB, post-processed: Unknown size, total: 11.72 MiB) to /root/.cache/huggingface/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13718f10a5b74dc18197e5dedc8e8776"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0996d82dd31247d1965e3a0a8dc61e7a"}},"metadata":{}},{"name":"stdout","text":"Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n\n\n\n\n\n\nMax length of source sentence: 316\nMax length of target sentence: 287\nNo model to preload, starting from scratch\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 00: 100%|██████████| 1213/1213 [17:24<00:00,  1.16it/s, loss=6.217]\n","output_type":"stream"},{"name":"stdout","text":"    SOURCE: 'As candidate for the post of Provincial Marshal, Captain Eugene Ivanich Apukhtin will now be balloted for.'\n    TARGET: — È proposto come candidato alla carica di maresciallo del governatorato il capitano in seconda di cavalleria Evgenij Ivanovic Apuchtin!\n PREDICTED: \n    SOURCE: No one else was in the church except a soldier-beggar, two old women, and the clergy.\n    TARGET: In chiesa non c’era nessuno all’infuori di un povero soldato, due vecchiette e i sacrestani.\n PREDICTED: \n      BLEU: 0.0\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 01: 100%|██████████| 1213/1213 [17:28<00:00,  1.16it/s, loss=5.496]\n","output_type":"stream"},{"name":"stdout","text":"    SOURCE: This plan would be all the more convenient because the young couple intended immediately after the wedding to go to the country, where the larger part of the trousseau would not be required.\n    TARGET: Una decisione simile era quanto mai opportuna, perché la giovane coppia, subito dopo il matrimonio, si sarebbe recata in campagna, dove il corredo di casa non sarebbe stato necessario.\n PREDICTED: \n    SOURCE: I love you, and it's all the same to me,' she said, changing from French into Russian, while her eyes as she looked at him glittered with a light he could not understand, 'so long as you have not changed toward me!\n    TARGET: Io ti amo e per me tutto il resto è indifferente — ella disse in russo, guardandolo con uno scintillio particolare, per lui incomprensibile — se tu non sei cambiato.\n PREDICTED: \n      BLEU: 0.0\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 02: 100%|██████████| 1213/1213 [17:29<00:00,  1.16it/s, loss=5.234]\n","output_type":"stream"},{"name":"stdout","text":"    SOURCE: Different as were those two women, Agatha Mikhaylovna and Kitty – or 'Kate' as Nicholas called her, and as Levin was also fond of calling her now – in that respect they were exactly alike.\n    TARGET: Per quanto diverse fossero queste due donne, Agaf’ja Michajlovna e Katja, come la chiamava suo fratello Nikolaj, e come adesso era particolarmente caro per Levin chiamarla, in questo erano perfettamente simili.\n PREDICTED: \n    SOURCE: \"When I go to India, Jane, will I leave you!\n    TARGET: — Quando andrò nell'India vi lascierò forse, Jane?\n PREDICTED: \n      BLEU: 0.0\n","output_type":"stream"},{"name":"stderr","text":"Processing Epoch 03:   1%|          | 9/1213 [00:07<17:16,  1.16it/s, loss=5.268]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"Ext_g_Z1viVV"},"execution_count":null,"outputs":[]}]}