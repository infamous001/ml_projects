{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gXyAal9U6bmU"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self,dim:int,eps:float=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.weight=nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self,x:torch.Tensor):\n",
        "    #x->[b,d,s] 1/sqrt(mean(x^2))->[b,d,s] x*1/sqrt()->[b,d,s]\n",
        "    return x*torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps) #torch.rsqrt(x)=1/root(x)\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    return self.weight*self._norm(x.float()).type_as(x)"
      ],
      "metadata": {
        "id": "F_VF09g5EkmL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_theta_pos_frequencies(head_dim:int,seq_len:int,device:str,theta:float=10000.0):\n",
        "  #head_dim is dim after distributing to heads\n",
        "  assert head_dim%2==0,\"head dim must be even number as defined in original repo\"\n",
        "  theta_numerator=torch.arange(0,head_dim,2).float() # this 2(i-1) part->[0,2,4,6,8...]\n",
        "  theta=1.0/(theta**(theta_numerator/head_dim)).to(device) #1/10000^(theta_num)\n",
        "  m=torch.arange(seq_len,device=device) #m=[0,1,2,3,4...]\n",
        "    # m->seq_len theta->head_dim/2\n",
        "    # m outer theta ->[seq_len,head_dim/2]\n",
        "    # a=[1,2,3] b=[1,2,3] -> [[1,2,3],[2,4,6],[3,6,9]]\n",
        "    # every m is multiplied to every value of thetha\n",
        "  freqs=torch.outer(m,theta).float()\n",
        "    #convert [1,angle] -> [1*cos(angle)+i(1*sin(angle))]\n",
        "  freqs_complex=torch.polar(torch.ones_like(freqs),freqs)\n",
        "  return freqs_complex"
      ],
      "metadata": {
        "id": "4htJLrPMKMHY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_embeddings(x:torch.Tensor,freqs_complex:torch.Tensor,device:str):\n",
        "  #[b,s,h,hdim]->[b,s,h,hdim/2,2]->convert to complex->[b,s,h,hdim/2]\n",
        "  x_complex=torch.view_as_complex(x.float().reshape(*x.shape[:-1],-1,2)) # Two consecutive values will become a single complex number\n",
        "  #[s,h_dim/2]->[1,s,1,h_dim] (b,s,h,h_dim)\n",
        "  freqs_complex=freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  #[x1+ix2]*[cos(m1theta+i sin(m1thetha))] [b,s,h,h_dim/2]*[1,s,1,h_dim/2]->[b,s,h,h_dim/2]\n",
        "  x_rotated=x_complex*freqs_complex\n",
        "  #[x1+ix2]->[x1,x2] [b,s,h,h_dim/2]->[b,s,h,h_dim/2,2]\n",
        "  x_out=torch.view_as_real(x_rotated)\n",
        "  #[b,s,h,h_dim/2,2]->[b,s,h,h_dim]\n",
        "  x_out=x_out.reshape(*x.shape)\n",
        "  return x_out.type_as(x).to(device)"
      ],
      "metadata": {
        "id": "wRi9RKobFeWQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x:torch.Tensor,n_rep:int):\n",
        "  batch_size,seq_len,n_kv_heads,head_dim=x.shape\n",
        "  if(n_rep==1):\n",
        "    return x\n",
        "  else:\n",
        "    return (\n",
        "        x[:,:,:,None,:]#[b,s,h,d]->[b,s,h,1,d]\n",
        "        .expand(batch_size,seq_len,n_kv_heads,n_rep,head_dim)#[b,s,h,1,d]->[b,s,h,n_rep,d]\n",
        "        .reshape(batch_size,seq_len,n_kv_heads*n_rep,head_dim)#[b,s,h,n_rep,d]->[b,s,h*n_rep,d]\n",
        "    )"
      ],
      "metadata": {
        "id": "gG0mPJ-FFiPz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model args\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim:int=4096\n",
        "  n_layers:int=32\n",
        "  n_heads:int=32\n",
        "  n_kv_heads:Optional[int]=None\n",
        "  vocab_size:int=-1 #will be loaded by build function from original weights of model\n",
        "  multiple_of:int=256 #hidden dim of ff_layer will be multiple of this\n",
        "  ff_dim_multiplier:Optional[float]=None #to make hidden_dim a closest multiplier of this\n",
        "  norm_eps:float=1e-5\n",
        "\n",
        "  #for kv cache\n",
        "  max_batch_size=32\n",
        "  max_seq_len=2048\n",
        "\n",
        "  device:str=None"
      ],
      "metadata": {
        "id": "HhqRak8GCOhl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    self.heads_q=args.n_heads\n",
        "    self.n_kv_heads=args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    self.head_dim=args.dim//args.n_heads\n",
        "    self.n_rep=self.n_heads_q//self.n_kv_heads\n",
        "\n",
        "    self.wq=nn.Linear(args.dim,args.n_heads*self.head_dim,bias=False)\n",
        "    self.wk=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
        "    self.wq=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
        "    self.wo=nn.Linear(args.n_heads*self.head_dim,args.dim,bias=False)\n",
        "\n",
        "    #to store k and v\n",
        "    self.cache_k=torch.zeros((args.max_batch_size,args.max_seq_len,self.n_kv_heads,self.head_dim))\n",
        "    self.cache_v=torch.zeros((args.max_batch_size,args.max_seq_len,self.n_kv_heads,self.head_dim))\n",
        "\n",
        "  def forward(self,x:torch.Tensor,start_pos:int,freqs_complex:torch.Tensor):\n",
        "    batch_size,seq_len,_=x.shape #x=[b,s,d]\n",
        "\n",
        "    #s=1 as we deal with one token at a time\n",
        "    xq=self.wq(x) #[b,s,d]->[b,s,h*h_dim]\n",
        "    xk=self.wk(x) #[b,s,d]->[b,s,h_kv*h_dim]\n",
        "    xv=self.wv(x) #[b,s,d]->[b,s,h_kv*h_dim]\n",
        "\n",
        "    #[b,s,h*h_dim]->[b,s,h,h_dim]\n",
        "    xq=xq.view(batch_size,seq_len,self.n_heads_q,self.head_dim)\n",
        "    #[b,s,h_kv*h_dim]->[b,s,h_kv,h_dim]\n",
        "    xk=xk.view(batch_size,seq_len,self.n_kv_heads,self.head_dim)\n",
        "    #[b,s,h_kv*h_dim]->[b,s,h_kv,h_dim]\n",
        "    xv=xv.view(batch_size,seq_len,self.n_kv_heads,self.head_dim)\n",
        "\n",
        "    #[b,s,h_q,h_dim]->[b,s,h_q,h_dim]\n",
        "    xq=apply_rotary_embeddings(xq,freqs_complex,x.device)\n",
        "    #[b,s,h_kv,h_dim]->[b,s,h_kv,h_dim]\n",
        "    xk=apply_rotary_embeddings(xk,freqs_complex,x.device)\n",
        "\n",
        "    #put new entry in kv_cache\n",
        "    self.cache_k[:batch_size,start_pos:start_pos+seq_len]=xk\n",
        "    self.cache_v[:batch_size,start_pos:start_pos+seq_len]=xv\n",
        "\n",
        "    #gets the k and v from cache\n",
        "    #keys->[b,s_kv,h_kv,h_dim]\n",
        "    keys=self.cache_k[:batch_size,:start_pos+seq_len]\n",
        "    #values->[b,s_kv,h_kv,h_dim]\n",
        "    values=self.cache_v[:batch_size,:start_pos+seq_len]\n",
        "\n",
        "    #[b,s_kv,h_kv,h_dim]->[b,s_kv,h_kv*n_rep,h_dim]\n",
        "    keys=repeat_kv(keys,self.n_rep)\n",
        "    #[b,s_kv,h_kv,h_dim]->[b,s_kv,h_kv*n_rep,h_dim]\n",
        "    values=repeat_kv(values,self.n_rep)\n",
        "\n",
        "    #[b,s,h_q,h_dim]->[b,h_q,s,h_dim] s=1\n",
        "    xq=xq.transpose(1,2)\n",
        "    #[b,s_kv,h_q,h_dim]->[b,h_q,s_kv,h_dim]\n",
        "    keys=keys.transpose(1,2)\n",
        "    #[b,s_kv,h_q,h_dim]->[b,h_q,s_kv,h_dim]\n",
        "    values=values.transpose(1,2)\n",
        "\n",
        "    # k=[b,h_q,s_kv,h_dim]->[b,h_q,h_dim,s_kv]\n",
        "    #[b,h_q,s,h_dim]*[b,h_q,h_dim,s_kv]=[b,h_q,s,s_kv]\n",
        "    score=torch.matmul(xq,keys.transpose(2,3))/math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "    #[b,h_q,s,s_kv]*[b,h_q,s_kv,h_dim]->[b,h,s,h_dim]->[b,s,h,h_dim]->[b,s,dim]\n",
        "    output=torch.matmul(scores,values)\n",
        "    output=(output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
        "    return self.wo(output)"
      ],
      "metadata": {
        "id": "4z40cpnwRLVE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    #we are doing all this to increase the parameters in model\n",
        "    hidden_dim=4*args.dim\n",
        "    hidden_dim=int(2*hidden_dim/3)\n",
        "    if args.ffn_dim_multiplier is not None:\n",
        "      hidden_dim=int(args.ffn_dim_multiplier*hidden_dim)\n",
        "    #round off to next multiple of multipier\n",
        "    hidden_dim=args.multiple_of*((hidden_dim+args.multiple_of-1)//args.multiple_of)\n",
        "    # hidden 7 round off to multiple of 5\n",
        "    #7+5-1=11 11//5=2. 2*5=10\n",
        "\n",
        "    self.w1=nn.Linear(args.dim,hidden_dim,bias=False)\n",
        "    self.w3=nn.Linear(hidden_dim,args.dim,bias=False)\n",
        "    self.w3=nn.Linear(args.dim,hidden_dim,bias=False)\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    swish=F.silu(self.w1(x))\n",
        "    x_V=self.w3(x)\n",
        "    x=swish*x_V\n",
        "    x=self.w2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "o5sxq_ahR4xc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    self.n_heads=args.n_heads\n",
        "    self.dim=args.dim\n",
        "    self.head_dim=args.dim//args.n_heads\n",
        "\n",
        "    self.attention=SelfAttention(args)\n",
        "    self.feed_forward=FeedForward(args)\n",
        "\n",
        "    self.attention_norm=RMSNorm(self.dim,eps=args.norm_eps)\n",
        "    self.ffn_norm=RMSNorm(self.dim,eps=args.norm_eps)\n",
        "\n",
        "  def forward(self,x:torch.Tensor,start_pos:int,freqs_complex:torch.Tensor):\n",
        "    h=x+self.attention.forward(self.attention_norm(x),start_pos,freqs_complex)\n",
        "    out=h+self.feed_forward.forward(self.ffn_norm(h))\n",
        "    return out"
      ],
      "metadata": {
        "id": "vWVNEwp5wo0R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    assert args.vocab_size!=-1,\"vocab size must be set\"\n",
        "\n",
        "    self.args=args\n",
        "    self.vocab_size=args.vocab_size\n",
        "    self.n_layers=args.n_layers\n",
        "\n",
        "    self.tok_embedding=nn.Embedding(self.vocab_size,args.dim)\n",
        "\n",
        "    self.layers=nn.ModuleList()\n",
        "    for layer in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self,tokens:torch.Tensor,start_pos:int):\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"Only one token at a time can be processed\"\n",
        "\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "    output = self.output(h).float()\n",
        "    return output"
      ],
      "metadata": {
        "id": "iPQG3vLpzGDl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sentencepiece import SentencePieceProcessor"
      ],
      "metadata": {
        "id": "qglBX2c83u4S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLaMA:\n",
        "  def __init__(self,model:Transformer,tokenizer:SentencePieceProcessor,model_args:ModelArgs):\n",
        "    self.model=model,\n",
        "    self.tokeizer=tokenizer,\n",
        "    self.args=model_args\n",
        "\n",
        "  @staticmethod\n",
        "  def build(checkpoints_dir:str,tokenizer_path:str,load_model:bool,max_seq_len:int,max_batch_size:int,device:str):\n",
        "    if load_model:\n",
        "      checkpoints=sorted(Path(checkpoints_dir).glob(\"*.pth\"))#loads all files with .pth and sort them\n",
        "      assert len(checkpoints) > 0, f\"no checkpoint files found mf\"\n",
        "      ckpt_path=checkpoints[0]\n",
        "      print(\"loading checkpoints\")\n",
        "      checkpoint=torch.load(ckpt_path,map_location=\"cpu\")\n",
        "    with open(Path(checkpoints_dir) / \"params.json\", \"r\") as f:\n",
        "      params = json.loads(f.read())\n",
        "\n",
        "    model_args=ModelArgs(\n",
        "        max_seq_len=max_seq_len,\n",
        "        max_batch_size=max_batch_size,\n",
        "        device=device,\n",
        "        **params\n",
        "    )\n",
        "    tokenizer=SentencePieceProcessor()#intialize tokeizer\n",
        "    tokenizer.load(tokenizer_path)#load it with pretrained weights\n",
        "    model_args.vocab_size=tokenizer.vocab_size()\n",
        "\n",
        "    if device == \"cuda\":\n",
        "      torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
        "    else:\n",
        "      torch.set_default_tensor_type(torch.BFloat16Tensor)\n",
        "\n",
        "    model=Transformer(model_args)#intilalize model\n",
        "\n",
        "    if load_model:\n",
        "    # The only unmatched key in the checkpoint is rope.freqs. Remove it\n",
        "      del checkpoint['rope.freqs']\n",
        "      model.load_state_dict(checkpoint, strict=True) #strict make sure all the varibales have same name\n",
        "\n",
        "    return LLaMA(model,tokenizer,model_args)\n",
        "\n",
        "  def text_completion(self, prompts: list[str], temperature: float = 0.6, top_p: float = 0.9, max_gen_len: Optional[int] = None):\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.args.max_seq_len - 1\n",
        "        # Convert each prompt into tokens\n",
        "        prompt_tokens = [self.tokenizer.encode(prompt, out_type=int, add_bos=True, add_eos=False) for prompt in prompts]\n",
        "        # Make sure the batch size is not too large\n",
        "        batch_size = len(prompt_tokens)\n",
        "        assert batch_size <= self.args.max_batch_size, f\"batch size must be less than or equal to {self.args.max_batch_size}\"\n",
        "        max_prompt_len = max(len(prompt) for prompt in prompt_tokens)\n",
        "        # Make sure the prompt length is not larger than the maximum sequence length\n",
        "        assert max_prompt_len <= self.args.max_seq_len, f\"prompt length must be less than or equal to {self.args.max_seq_len}\"\n",
        "        total_len = min(self.args.max_seq_len, max_gen_len + max_prompt_len)\n",
        "\n",
        "        # Create the list that will contain the generated tokens, along with the initial prompt tokens\n",
        "        pad_id = self.tokenizer.pad_id()\n",
        "        tokens = torch.full((batch_size, total_len), pad_id, dtype=torch.long, device=device)\n",
        "        for k, t in enumerate(prompt_tokens):\n",
        "            # Populate the initial tokens with the prompt tokens\n",
        "            tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device=device)\n",
        "\n",
        "        eos_reached = torch.tensor([False] * batch_size, device=device)\n",
        "        prompt_tokens_mask = tokens != pad_id # True if the token is a prompt token, False otherwise\n",
        "        cur_iterator = tqdm(range(1, total_len), desc=\"Generating tokens\")\n",
        "        for cur_pos in cur_iterator:\n",
        "            with torch.no_grad():\n",
        "                logits = self.model.forward(tokens[:, cur_pos-1:cur_pos], cur_pos)\n",
        "            if temperature > 0:\n",
        "                # The temperature is applied before the softmax\n",
        "                probs = torch.softmax(logits[:, -1] / temperature, dim=-1)\n",
        "                next_token = self._sample_top_p(probs, top_p)\n",
        "            else:\n",
        "                # Greedily select the token with the max probability\n",
        "                next_token = torch.argmax(logits[:, -1], dim=-1)\n",
        "\n",
        "            next_token = next_token.reshape(-1)\n",
        "            # Only replace token if it is a padding token\n",
        "            next_token = torch.where(prompt_tokens_mask[:, cur_pos], tokens[:, cur_pos], next_token)\n",
        "            tokens[:, cur_pos] = next_token\n",
        "            # EOS is reached only if we found an EOS token for a padding position\n",
        "            eos_reached |= (~prompt_tokens_mask[:, cur_pos]) & (next_token == self.tokenizer.eos_id)\n",
        "            if all(eos_reached):\n",
        "                break\n",
        "        out_tokens = []\n",
        "        out_text = []\n",
        "        for prompt_index, current_prompt_tokens in enumerate(tokens.tolist()):\n",
        "            # Cut to the EOS token, if present\n",
        "            if self.tokenizer.eos_id in current_prompt_tokens:\n",
        "                eos_idx = current_prompt_tokens.index(self.tokenizer.eos_id)\n",
        "                current_prompt_tokens = current_prompt_tokens[:eos_idx]\n",
        "            out_tokens.append(current_prompt_tokens)\n",
        "            out_text.append(self.tokenizer.decode(current_prompt_tokens))\n",
        "        return (out_tokens, out_text)\n",
        "\n",
        "  def _sample_top_p(self, probs, p):\n",
        "        # (B, vocab_size)\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "        # (B, vocab_size)\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "        # (B, vocab_size)\n",
        "        # (Substracting \"probs_sort\" shifts the cumulative sum by 1 position to the right before masking)\n",
        "        mask = probs_sum - probs_sort > p\n",
        "        # Zero out all the probabilities of tokens that are not selected by the Top P\n",
        "        probs_sort[mask] = 0.0\n",
        "        # Redistribute the probabilities so that they sum up to 1.\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "        # Sample a token (its index) from the top p distribution\n",
        "        next_token = torch.multinomial(probs_sort, num_samples=1)\n",
        "        # Get the token position in the vocabulary corresponding to the sampled index\n",
        "        next_token = torch.gather(probs_idx, -1, next_token)\n",
        "        return next_token\n"
      ],
      "metadata": {
        "id": "5Cvl6K8RolPX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    allow_cuda = False\n",
        "    device = 'cuda' if torch.cuda.is_available() and allow_cuda else 'cpu'\n",
        "\n",
        "    prompts = [\n",
        "        \"What is Mark Zuckerberg was born in india \",\n",
        "        # Few shot promt\n",
        "        \"\"\"Translate English to French:\n",
        "\n",
        "        goodmorning=>bonjour\n",
        "        peppermint => menthe poivrée\n",
        "        plush girafe => girafe peluche\n",
        "        cheese =>\"\"\",\n",
        "    ]\n",
        "\n",
        "    model = LLaMA.build(\n",
        "        checkpoints_dir='llama-2-7b/',\n",
        "        tokenizer_path='tokenizer.model',\n",
        "        load_model=True,\n",
        "        max_seq_len=1024,\n",
        "        max_batch_size=len(prompts),\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    out_tokens, out_texts = (model.text_completion(prompts, max_gen_len=64))\n",
        "    assert len(out_texts) == len(prompts)\n",
        "    for i in range(len(out_texts)):\n",
        "        print(f'{out_texts[i]}')\n",
        "        print('-' * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iykIdklvubXN",
        "outputId": "0a5437d1-7289-43a9-9360-aa94dab18118"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRINfcb4usrX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}