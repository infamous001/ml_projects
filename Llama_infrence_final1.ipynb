{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "gXyAal9U6bmU"
      },
      "outputs": [],
      "source": [
        "#libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from typing import Optional\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self,dim:int,eps:float=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.weight=nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def _norm(self,x:torch.Tensor):\n",
        "    #x->[b,d,s] 1/sqrt(mean(x^2))->[b,d,s] x*1/sqrt()->[b,d,s]\n",
        "    return x*torch.rsqrt(x.pow(2).mean(-1,keepdim=True)+self.eps) #torch.rsqrt(x)=1/root(x)\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    return self.weight*self._norm(x.float()).type_as(x)"
      ],
      "metadata": {
        "id": "F_VF09g5EkmL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_theta_pos_frequencies(head_dim:int,seq_len:int,device:str,theta:float=10000.0):\n",
        "  #head_dim is dim after distributing to heads\n",
        "  assert head_dim%2==0,\"head dim must be even number as defined in original repo\"\n",
        "  theta_numerator=torch.arange(0,head_dim,2).float() # this 2(i-1) part->[0,2,4,6,8...]\n",
        "  theta=1.0/(theta**(theta_numerator/head_dim)).to(device) #1/10000^(theta_num)\n",
        "  m=torch.arange(seq_len,device=device) #m=[0,1,2,3,4...]\n",
        "    # m->seq_len theta->head_dim/2\n",
        "    # m outer theta ->[seq_len,head_dim/2]\n",
        "    # a=[1,2,3] b=[1,2,3] -> [[1,2,3],[2,4,6],[3,6,9]]\n",
        "    # every m is multiplied to every value of thetha\n",
        "  freqs=torch.outer(m,theta).float()\n",
        "    #convert [1,angle] -> [1*cos(angle)+i(1*sin(angle))]\n",
        "  freqs_complex=torch.polar(torch.ones_like(freqs),freqs)\n",
        "  return freqs_complex"
      ],
      "metadata": {
        "id": "4htJLrPMKMHY"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rotary_embeddings(x:torch.Tensor,freqs_complex:torch.Tensor,device:str):\n",
        "  #[b,s,h,hdim]->[b,s,h,hdim/2,2]->convert to complex->[b,s,h,hdim/2]\n",
        "  x_complex=torch.view_as_complex(x.float().reshape(*x.shape[:-1],-1,2)) # Two consecutive values will become a single complex number\n",
        "  #[s,h_dim/2]->[1,s,1,h_dim] (b,s,h,h_dim)\n",
        "  freqs_complex=freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "  #[x1+ix2]*[cos(m1theta+i sin(m1thetha))] [b,s,h,h_dim/2]*[1,s,1,h_dim/2]->[b,s,h,h_dim/2]\n",
        "  x_rotated=x_complex*freqs_complex\n",
        "  #[x1+ix2]->[x1,x2] [b,s,h,h_dim/2]->[b,s,h,h_dim/2,2]\n",
        "  x_out=torch.view_as_real(x_rotated)\n",
        "  #[b,s,h,h_dim/2,2]->[b,s,h,h_dim]\n",
        "  x_out=x_out.reshape(*x.shape)\n",
        "  return x_out.type_as(x).to(device)"
      ],
      "metadata": {
        "id": "wRi9RKobFeWQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x:torch.Tensor,n_rep:int):\n",
        "  batch_size,seq_len,n_kv_heads,head_dim=x.shape\n",
        "  if(n_rep==1):\n",
        "    return x\n",
        "  else:\n",
        "    return (\n",
        "        x[:,:,:,None,:]#[b,s,h,d]->[b,s,h,1,d]\n",
        "        .expand(batch_size,seq_len,n_kv_heads,n_rep,head_dim)#[b,s,h,1,d]->[b,s,h,n_rep,d]\n",
        "        .reshape(batch_size,seq_len,n_kv_heads*n_rep,head_dim)#[b,s,h,n_rep,d]->[b,s,h*n_rep,d]\n",
        "    )"
      ],
      "metadata": {
        "id": "gG0mPJ-FFiPz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model args\n",
        "@dataclass\n",
        "class ModelArgs:\n",
        "  dim:int=4096\n",
        "  n_layers:int=32\n",
        "  n_heads:int=32\n",
        "  n_kv_heads:Optional[int]=None\n",
        "  vocab_size:int=-1 #will be loaded by build function from original weights of model\n",
        "  multiple_of:int=256 #hidden dim of ff_layer will be multiple of this\n",
        "  ff_dim_multiplier:Optional[float]=None #to make hidden_dim a closest multiplier of this\n",
        "  norm_eps:float=1e-5\n",
        "\n",
        "  #for kv cache\n",
        "  max_batch_size=32\n",
        "  max_seq_len=2048\n",
        "\n",
        "  device:str=None"
      ],
      "metadata": {
        "id": "HhqRak8GCOhl"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    self.heads_q=args.n_heads\n",
        "    self.n_kv_heads=args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "    self.head_dim=args.dim//args.n_heads\n",
        "    self.n_rep=self.n_heads_q//self.n_kv_heads\n",
        "\n",
        "    self.wq=nn.Linear(args.dim,args.n_heads*self.head_dim,bias=False)\n",
        "    self.wk=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
        "    self.wq=nn.Linear(args.dim,self.n_kv_heads*self.head_dim,bias=False)\n",
        "    self.wo=nn.Linear(args.n_heads*self.head_dim,args.dim,bias=False)\n",
        "\n",
        "    #to store k and v\n",
        "    self.cache_k=torch.zeros((args.max_batch_size,args.max_seq_len,self.n_kv_heads,self.head_dim))\n",
        "    self.cache_v=torch.zeros((args.max_batch_size,args.max_seq_len,self.n_kv_heads,self.head_dim))\n",
        "\n",
        "  def forward(self,x:torch.Tensor,start_pos:int,freqs_complex:torch.Tensor):\n",
        "    batch_size,seq_len,_=x.shape #x=[b,s,d]\n",
        "\n",
        "    #s=1 as we deal with one token at a time\n",
        "    xq=self.wq(x) #[b,s,d]->[b,s,h*h_dim]\n",
        "    xk=self.wk(x) #[b,s,d]->[b,s,h_kv*h_dim]\n",
        "    xv=self.wv(x) #[b,s,d]->[b,s,h_kv*h_dim]\n",
        "\n",
        "    #[b,s,h*h_dim]->[b,s,h,h_dim]\n",
        "    xq=xq.view(batch_size,seq_len,self.n_heads_q,self.head_dim)\n",
        "    #[b,s,h_kv*h_dim]->[b,s,h_kv,h_dim]\n",
        "    xk=xk.view(batch_size,seq_len,self.n_kv_heads,self.head_dim)\n",
        "    #[b,s,h_kv*h_dim]->[b,s,h_kv,h_dim]\n",
        "    xv=xv.view(batch_size,seq_len,self.n_kv_heads,self.head_dim)\n",
        "\n",
        "    #[b,s,h_q,h_dim]->[b,s,h_q,h_dim]\n",
        "    xq=apply_rotary_embeddings(xq,freqs_complex,x.device)\n",
        "    #[b,s,h_kv,h_dim]->[b,s,h_kv,h_dim]\n",
        "    xk=apply_rotary_embeddings(xk,freqs_complex,x.device)\n",
        "\n",
        "    #put new entry in kv_cache\n",
        "    self.cache_k[:batch_size,start_pos:start_pos+seq_len]=xk\n",
        "    self.cache_v[:batch_size,start_pos:start_pos+seq_len]=xv\n",
        "\n",
        "    #gets the k and v from cache\n",
        "    #keys->[b,s_kv,h_kv,h_dim]\n",
        "    keys=self.cache_k[:batch_size,:start_pos+seq_len]\n",
        "    #values->[b,s_kv,h_kv,h_dim]\n",
        "    values=self.cache_v[:batch_size,:start_pos+seq_len]\n",
        "\n",
        "    #[b,s_kv,h_kv,h_dim]->[b,s_kv,h_kv*n_rep,h_dim]\n",
        "    keys=repeat_kv(keys,self.n_rep)\n",
        "    #[b,s_kv,h_kv,h_dim]->[b,s_kv,h_kv*n_rep,h_dim]\n",
        "    values=repeat_kv(values,self.n_rep)\n",
        "\n",
        "    #[b,s,h_q,h_dim]->[b,h_q,s,h_dim] s=1\n",
        "    xq=xq.transpose(1,2)\n",
        "    #[b,s_kv,h_q,h_dim]->[b,h_q,s_kv,h_dim]\n",
        "    keys=keys.transpose(1,2)\n",
        "    #[b,s_kv,h_q,h_dim]->[b,h_q,s_kv,h_dim]\n",
        "    values=values.transpose(1,2)\n",
        "\n",
        "    # k=[b,h_q,s_kv,h_dim]->[b,h_q,h_dim,s_kv]\n",
        "    #[b,h_q,s,h_dim]*[b,h_q,h_dim,s_kv]=[b,h_q,s,s_kv]\n",
        "    score=torch.matmul(xq,keys.transpose(2,3))/math.sqrt(self.head_dim)\n",
        "    scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "    #[b,h_q,s,s_kv]*[b,h_q,s_kv,h_dim]->[b,h,s,h_dim]->[b,s,h,h_dim]->[b,s,dim]\n",
        "    output=torch.matmul(scores,values)\n",
        "    output=(output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1))\n",
        "    return self.wo(output)"
      ],
      "metadata": {
        "id": "4z40cpnwRLVE"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    #we are doing all this to increase the parameters in model\n",
        "    hidden_dim=4*args.dim\n",
        "    hidden_dim=int(2*hidden_dim/3)\n",
        "    if args.ffn_dim_multiplier is not None:\n",
        "      hidden_dim=int(args.ffn_dim_multiplier*hidden_dim)\n",
        "    #round off to next multiple of multipier\n",
        "    hidden_dim=args.multiple_of*((hidden_dim+args.multiple_of-1)//args.multiple_of)\n",
        "    # hidden 7 round off to multiple of 5\n",
        "    #7+5-1=11 11//5=2. 2*5=10\n",
        "\n",
        "    self.w1=nn.Linear(args.dim,hidden_dim,bias=False)\n",
        "    self.w3=nn.Linear(hidden_dim,args.dim,bias=False)\n",
        "    self.w3=nn.Linear(args.dim,hidden_dim,bias=False)\n",
        "\n",
        "  def forward(self,x:torch.Tensor):\n",
        "    swish=F.silu(self.w1(x))\n",
        "    x_V=self.w3(x)\n",
        "    x=swish*x_V\n",
        "    x=self.w2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "o5sxq_ahR4xc"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    self.n_heads=args.n_heads\n",
        "    self.dim=args.dim\n",
        "    self.head_dim=args.dim//args.n_heads\n",
        "\n",
        "    self.attention=SelfAttention(args)\n",
        "    self.feed_forward=FeedForward(args)\n",
        "\n",
        "    self.attention_norm=RMSNorm(self.dim,eps=args.norm_eps)\n",
        "    self.ffn_norm=RMSNorm(self.dim,eps=args.norm_eps)\n",
        "\n",
        "  def forward(self,x:torch.Tensor,start_pos:int,freqs_complex:torch.Tensor):\n",
        "    h=x+self.attention.forward(self.attention_norm(x),start_pos,freqs_complex)\n",
        "    out=h+self.feed_forward.forward(self.ffn_norm(h))\n",
        "    return out"
      ],
      "metadata": {
        "id": "vWVNEwp5wo0R"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,args:ModelArgs):\n",
        "    super().__init__()\n",
        "    assert args.vocab_size!=-1,\"vocab size must be set\"\n",
        "\n",
        "    self.args=args\n",
        "    self.vocab_size=args.vocab_size\n",
        "    self.n_layers=args.n_layers\n",
        "\n",
        "    self.tok_embedding=nn.Embedding(self.vocab_size,args.dim)\n",
        "\n",
        "    self.layers=nn.ModuleList()\n",
        "    for layer in range(args.n_layers):\n",
        "      self.layers.append(EncoderBlock(args))\n",
        "\n",
        "    self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "    self.output = nn.Linear(args.dim, self.vocab_size, bias=False)\n",
        "\n",
        "    self.freqs_complex = precompute_theta_pos_frequencies(self.args.dim // self.args.n_heads, self.args.max_seq_len * 2, device=self.args.device)\n",
        "\n",
        "  def forward(self,tokens:torch.Tensor,start_pos:int):\n",
        "    batch_size, seq_len = tokens.shape\n",
        "    assert seq_len == 1, \"Only one token at a time can be processed\"\n",
        "\n",
        "    h = self.tok_embeddings(tokens)\n",
        "\n",
        "    # Retrieve the pairs (m, theta) corresponding to the positions [start_pos, start_pos + seq_len]\n",
        "    freqs_complex = self.freqs_complex[start_pos:start_pos + seq_len]\n",
        "\n",
        "    # Consecutively apply all the encoder layers\n",
        "    for layer in self.layers:\n",
        "      h = layer(h, start_pos, freqs_complex)\n",
        "    h = self.norm(h)\n",
        "    output = self.output(h).float()\n",
        "    return output"
      ],
      "metadata": {
        "id": "iPQG3vLpzGDl"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qglBX2c83u4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}